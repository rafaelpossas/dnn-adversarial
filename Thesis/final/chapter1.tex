\chapter{Introduction}
Pattern Recognition and Data Mining is a field of study focused on using relatively complex algorithms to discover knowledge from large pools of data. These are used to predict the future or to recognise patterns and label data points that are close together. The increase of computational power on the last two decades leveraged the use of techniques such as Neural Networks \cite{bishop1995neural} to tackle more complex problems like automatic labeling of digital images. The work of Lecun (1989) \cite{lecunn89} was one of the stepping stones for all work on image recognition using Neural Networks as he was able to prove the effectiveness of stacking multiple layers of neurons to form what we call a Neural Network. These studies usually refer to how the human brain works to explain the inspiration of such techniques and each neuron was later named as perceptrons.

The focus on the early days of neural networks was to understand the main principles behind human learning. For instance, recognizing handwritten digits could be seen as a trivial and effortless job for most people, however, making a computer to be able to perform this same task was not as easy as it seemed. By discovering the pattern behind digit recognition, computers would also be able to start understanding broader classes of images \cite{krizhevsky2012}. The use of computer vision techniques along with machine learning algorithms are nowadays the state of the art techniques to overcome these challenges.

Computer Vision is a field of study that focuses on processing digital images and has Neural Networks as one of the most used predictive techniques. These algorithms are focused on learning models that recognise patterns on data with several dimensions (e.g. images with width x height number of pixels). Recent advancements on both Computer Vision and Neural Networks have led to the development of a new class of algorithms which is nowadays known as either Deep Learning or Deep Feed Forward Networks.

Extremely deep networks (e.g. one containing stacked perceptrons layers) are categorized as deep learning algorithms and can be more popularly represented through deep feed forward neural networks \cite{hornik1989multilayer} and, more recently, recursive neural networks \cite{goller1996learning}. While the latter focuses on solving problems where data points are dependent on one another (e.g. Time series) the first is more largely used on image recognition and, therefore, should be the focus of this work. A more specific approach on deep feed forward nets is to extract important features from images before trying to classify them as a predefined class. This approach is widely used on a specific Neural Network architecture called Convolutional Neural Networks (CNNs) \cite{matsugu2003subject}.

Recent work has shown that the CNNs generalisation capabilities are not as good as most people thought. The work of \cite{goodfellow2014} has shown that although these models provide very high accuracy on complex task, their underlying learning structure is rather sparse \cite{papernot2016}. This sparsity enables methods to intentionally fool the network into predicting a different class for a given image. This operation is achieved by adding enough directed noise to each pixel of an image in order to fool an algorithm into thinking that the image has a different label \cite{goodfellow2014}\cite{papernot2016transf}\cite{goodfellow2016}\cite{szegedy2013}. The resulting images of this process are known as adversarial examples and their generation can be mainly done through the use of a method called gradient sign \cite{goodfellow2014}.

The imbalanced learning is a well known cause for lower performance of several machine learning algorithms. Data distribution on real world is usually skewed and rarely contains enough information to learn all the necessary features of the data domain. As the dependency of specific class labels grows, the vulnerability of the system increases, as wrong predictions of the critical label could lead to unwanted outcomes. Adversarial examples are known for forcing domain shift on classes with similar distributions and, hence, are able to change the classification labels of test inputs. The imbalanced class problem could then become more critical when the use of such technique happens to models where the data domain presents the aforementioned characteristics.

This thesis presents an experimental study aimed to understand the factors leading to the robustness to adversarial attacks of  class imbalanced CNNs. Through the use of the fast gradient sign method we investigate how a skewed data distribution could affect the network robustness to such attacks. Understanding such threat vectors is of the utmost importance for the development of current techniques, as it not only drives the creation of new regularization techniques but also helps current commercial systems to protect themselves from these attacks.


\section{Motivation}

To date, little evidence has been found associating training characteristics of a model with its robustness to adversarial inputs. Experimental demonstrations of adversarial effects were carried out mainly by the deep learning and computer vision community. The motivation for adversarial robustness comes largely from being able to shield image recognition systems from behaving unexpectedly. Previous published studies \cite{papernot2016}, \cite{goodfellow2014}, \cite{billovits} are limited to show the general effectiveness of adversarial methods rather then understanding the deep relationship with the underlying training data distribution.

The world is embracing Artificial Intelligence and there are already a large number of solutions that rely on image recognition algorithms to perform tasks that range from car plate recognition to more general machine learning APIs such as Amazon Rekognition. In particular, computer vision and up to date machine learning algorithms could greatly benefit from understanding the relationship between the imbalanced learning problem and adversarial attacks.

\section{Thesis Goals}
This study is set out to provide evidence on how models trained on skewed distributions could be more or less vulnerable to adversarial attacks. The main contributions of this work are as follows:
\begin{itemize}
	\item To shed new light on how CNNs trained on imbalanced datasets are affected by adversarial attacks
	\item Evaluate the impact of transfer learning on imbalanced CNNs and how classes with similar set of features react to the perturbation caused by the gradient sign method
\end{itemize}

\section{Thesis Structure}

This thesis first gives a brief overview of the recent history of Neural Networks and the main related techniques and methods for using such networks on computer vision related tasks. Next chapter begins by introducing all the taxonomy around adversarial attacks followed by a theoretical discussion of domain shift in machine learning. Lastly it presents two variations of the gradient sign method and its different perturbation types. The fourth chapter presents an empirical study of machine learning transferability and provides a practical overview of different attacks to real world systems. The remaining chapters are concerned with explaining the experiment setup and presenting the achieved results and related future work.
