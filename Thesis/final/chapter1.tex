\chapter{Introduction}
Pattern Recognition and Data Mining is a field of study focused on using relatively complex algorithms to discover knowledge from large pools of data. These are usually used to predict the future or to recognise patterns and label data points that are close together. The increase of computational power on the last two decades leveraged the use of techniques such as Neural Networks \cite{bishop1995neural} to tackle more complex problems such as automatic labeling digital images. The work of Lecun (1989) \cite{lecunn89} was one of the stepping stones for all work on image recognition using Neural Networks as he was able to prove the effectiveness of stacking multiple layers of neurons to form what we call a Neural Network. These studies usually refer to how the human brain works to explain the inspiration of such techniques and each neuron was later named as perceptrons.

The efforts on the early days were focused in understanding the main principles behind human learning. For instance, recognizing handwritten digits could be seen as a trivial and effortless job for most people, however, making a computer to be able to perform this same task was not as easy as it seemed. By discovering the pattern behind digit recognition, computers would also be able to start understanding broader classes of images \cite{krizhevsky2012}. The use of computer vision techniques along with Machine Learning algorithms are nowadays the state of the art technique to overcome these challenges.

Computer Vision is a field of study that focuses on processing digital images and has Neural Networks as one of the underlying foundations for its algorithms. These are usually  focused on learning models that recognise patterns on data with several dimensions (e.g. images with width x height number of pixels). Recent advancements in both Computer Vision and Neural networks have led to the development of a new class of algorithms which is nowadays known as either Deep Learning or Deep Feed Forward Networks.

Extremely deep networks (e.g. one containing stacked perceptrons layers) are classified as Deep Learning algorithms and can be more popularly represented through deep feed forward neural networks \cite{hornik1989multilayer} and, more recently, recursive neural networks \cite{goller1996learning}. While the latter focuses on solving problems where the data points are dependent on one another (e.g. Time series) the first is more largely used on image recognition and, therefore, should be the focus of this work.A more specific approach on feed forward nets is to extract important features from images before trying to classify them as a predefined class. This approach is widely used on a specific Neural Network architecture called Convolutional Neural Networks \cite{matsugu2003subject}.

Recent work has shown that the generalization learned into those networks is rather sparse \cite{goodfellow2016}. This sparsity opens up an opportunity for methods that are able to go to unexplored data spaces in order to intentionally fool the network into predicting a different class for a given image. This operation is comprised of changing current images by adding just enough intentional noise to each pixel in order to fool an algorithm into thinking that the image has a different label \cite{goodfellow2014}\cite{papernot2016transf}\cite{goodfellow2016}\cite{szegedy2013}. The resulting images of this process are known as Adversarial Examples and their generation can be mainly done through the use of a method called Gradient Sign \cite{goodfellow2014}.

The imbalanced learning is a well known cause for lower performance of several machine learning algorithms. Data distribution on real world is usually skewed and rarely contains enough information to learn all the necessary features of the data domain. As the dependency of specific class labels grows, the vulnerability of the system increases, as wrong predictions of the critical label could lead to unwanted outcomes. Adversarial examples are known for forcing domain shift on classes with overlapping distributions and, hence, exploit some system vulnerabilities. The imbalanced class problem could then become more critical when the use of such technique happens to models where the data domain presents the aforementioned characteristics.

This thesis presents an experimental approach that aims to understand the factors leading to more or less robustness to adversarial attacks of the class imbalanced Convolutional Neural Networks. Through the use of the Fast Gradient Sign Method we will aim to understand how a skewed data distribution could affect the network robustness to such attacks. The understanding of such threat vectors is of the utmost importance for the development of current techniques, as it drives the creation of new regularization techniques.


\section{Motivation}

To date, little evidence has been found associating training characteristics of a model with its robustness to adversarial examples. Experimental demonstrations of adversarial effects were carried out mainly by the deep learning and computer vision community. The motivation for Adversarial robustness comes largely from being able to shield image recognition systems from behaving unexpectedly. Previous published studies are limited to showing the general effectiveness of adversarial methods rather then understanding the deep relationship with the underlying training data distribution.

The world is embracing Artificial Intelligence and there are already a large number of solutions that rely on image recognition algorithms to perform tasks that range from car plate recognition to more general machine learning APIs such as Amazon Rekognition. In particular, computer vision and up to date machine learning algorithms could greatly benefit from understanding the relationship between the imbalanced learning problem and adversarial attacks.

\section{Thesis Goals}
This study is set out to provide evidence on how models trained on skewed distributions could be more or less vulnerable to adversarial attacks. The main contributions of this work are as follows:
\begin{itemize}
	\item To shed new light on how the domain shift caused by adversaries is affected by imbalanced feature space exploration on Deep Neural Networks
	\item To fill gaps on literature regarding adversarial examples impacts on CNNs and how one could possibly increase the robustness to these attacks for specific class labels
	\item To specifically enhance understanding of Convolutional Neural Networks vulnerabilities and its implications to real world scenarios.
\end{itemize}

\section{Thesis Structure}

This thesis first gives a brief overview of the recent history of Neural Networks and the main related techniques and methods for using such networks on computer vision related tasks. Next chapter begins by introducing all the taxonomy around Adversarial Examples followed by a theoretical discussion of domain shift in machine learning. Lastly it presents two variations of the gradient sign method and its different perturbation types. The fourth chapter presents an empirical study of machine learning transferability and provides a practical overview of different attacks to real world systems. The remaining chapters are concerned with explaining the experiment setup and presenting the achieved results and related future work.
