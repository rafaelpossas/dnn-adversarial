\chapter{Conclusions and Future work}
Convolutional neural networks have been adopted more widely as the current techniques improve. However, understanding of such methods becomes harder when the number of parameters increase. Until now, most people  are used to see these models as black-boxes, since it is really hard to reason about what the model is actually learning. The excessive amount of parameters in these networks do not shield them from one of the most common effects in machine learning models - the domain shift caused by unseen data points.

Adversarial methods are proven to increase the domain shift effect on test datasets as it is explained by Papernot et al (2016) \cite{papernot2016transf}. We have shown that adversarial attacks are even more severe on datasets with under-sampled class labels and that the decision boundary trade-off on the over-sampled classes increases their robustness to adversarial examples. Labels with similar features have only shown higher vulnerability to the fast gradient sign methods in one of the classes of the pair. This specific result shows that similar classes might have degrees of similarities on which could be more or less exploited by the gradient method.  

This work sheds an important light on machine learning methods. Several real-life models are deeply concerned with possible vulnerabilities of their system, and studies on this field were being done for the past years. Still, the imbalance learning problem remains one of the big questions in machine learning.Current applications looking to increase their robustness to adversarial attacks could use over-sampling techniques on critical labels so as to shield that label from malicious attacks

As several commercial applications rely on almost the same group of models, understanding of such properties is of extreme importance. Future work in this field could look further in datasets with a higher number of classes and more complex relationships between labels so as to not only confirm our insights but also discover new interesting properties of class imbalanced CNNs and adversarial attacks. Current applications looking to increase their robustness to adversarial attacks could use over-sampling techniques on critical labels so as to shield that label from malicious attacks.