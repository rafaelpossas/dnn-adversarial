\chapter{Conclusion}

Adversarial methods are proven to increase the domain shift effect on test datasets as it is explained by Papernot et al (2016) \cite{papernot2016transf}. Yet, one should be careful when trying to interpret this shift without actually understanding how the feature space was actually explored. The imbalanced learning problem is intensified by Adversaries created with the same class gradient but it is not when the model is different. It should be noted that the effectiveness of the method dependence is twofold: how strong a specific class gradient is on the overall dataset and the space occupied by its distribution in space.

The right relationship between gradient strength and target class distribution is what causes networks to misclassify examples. A strong enough gradient would mean bigger steps towards other distributions and a small gradient would not have enough strength to move points from outside of their distributions. In order to keep perturbation constants across different models, one would need to tune $\epsilon$ accordingly in order to generate images with the lowest amount of noise possible and still get a successful attack. Until now, most of the studies in adversaries were aimed in the creation of methods that could effectively create those perturbations, however, one should aim to understand its characteristics more deeply as this not only helps to understand networks vulnerabilities but also provides new tools to explain the "black box" effects of deep neural nets.

This work sheds an important light on machine learning methods. Several real-life models are deeply concerned with possible vulnerabilities of their system, and studies on this field were being done for the past 20 years. Still, the imbalance learning problem remains one of the big questions in artificial intelligence. Adversarial attacks are one more concern for those working with such systems, but could also be seen as a tool that could forcibly make a model to stretch its occupation over the data domain space. Every new threat to predictive techniques is also a new improvement over model generalisation capabilities as the creation of the former could be used to improve the latter. Imbalanced datasets can be seen as one way of understanding adversarial attacks, as the lack of data space exploration seems to increase models vulnerabilities to such attacks

\section{Future Work}

Deep Neural Nets have been adopted more widely as the techniques improve. However, understanding of such methods becomes harder when the number of parameters increase. Until now, most people see these models as a black-box, since it is really hard to reason about what the model is actually learning. Tools like adversarial methods helps to extract insights from such methods and should be explored further. This work has performed tests on a dataset with a small number of classes, and in the future, datasets like ImageNet could be used to confirm or discover new insights. A good way of understanding DNN generalisation capabilities is by knowing what they are not able to do. The excessive amount of parameters in these networks do not shield them from one of the most common effects in machine learning models - the domain shift caused by unseen data points. We strongly believe that the studies of adversaries could greatly help this area of study and should be explored further.