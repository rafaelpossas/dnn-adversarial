\chapter{Introduction}
Pattern Recognition and Data Mining is a field of study focused on using relatively complex algorithms to discover knowledge from large pools of data. These are used to predict the future or to recognise patterns and label data points that are close together. The increase of computational power on the last two decades leveraged the use of techniques such as Neural Networks \cite{bishop1995neural} to tackle more complex problems like automatic labeling of digital images. The work of Lecun \cite{lecunn89} was one of the stepping stones for all work on image recognition using Neural Networks as he was able to prove the effectiveness of stacking multiple layers of neurons to form what we call a Neural Network. These studies usually refer to how the human brain works to explain the inspiration of such techniques and each neuron was later named as perceptrons.

The focus on the early days of neural networks was to understand the main principles behind human learning. For instance, recognizing handwritten digits could be seen as a trivial and effortless job for most people, however, making a computer to be able to perform this same task was not as easy as it seemed. By discovering the pattern behind digit recognition, computers would also be able to start understanding broader classes of images \cite{krizhevsky2012}. The use of computer vision techniques along with machine learning algorithms are nowadays the state of the art techniques to overcome these challenges.

Computer Vision is a field of study that focuses on processing digital images and has Neural Networks as one of the most used predictive techniques. These algorithms are focused on learning models that recognise patterns on data with several dimensions (e.g. images with width x height number of pixels). Recent advancements on both Computer Vision and Neural Networks have led to the development of a new class of algorithms which is nowadays known as either Deep Learning or Deep Feed Forward Networks.

Extremely deep networks (e.g. one containing stacked perceptrons layers) are categorized as deep learning algorithms and can be more popularly represented through deep feed forward neural networks \cite{hornik1989multilayer} and, more recently, recursive neural networks \cite{goller1996learning}. While the latter focuses on solving problems where data points are dependent on one another (e.g. Time series) the first is more largely used on image recognition and, therefore, should be the focus of this work. A more specific approach on deep feed forward nets is to extract important features from images before trying to classify them as a predefined class. This approach is widely used on a specific Neural Network architecture called Convolutional Neural Networks (CNNs) \cite{matsugu2003subject}.

Recent work has shown that CNNs have, opposed to common thinking, low generalisation capabilities. The work of \cite{goodfellow2014} has recently pointed out that although these models provide very high accuracy on complex task, their underlying learning structure is rather sparse \cite{papernot2016}. This sparsity enables methods to intentionally fool the network into predicting a different class for a given image. This operation is achieved by adding enough directed noise to each pixel of an image in order to fool an algorithm into thinking that the image has a different label \cite{goodfellow2014, papernot2016transf,goodfellow2016, szegedy2013}. The resulting images of this process are known as adversarial examples and their generation can be mainly done through the use of a method called gradient sign \cite{goodfellow2014}.

The imbalanced learning problem is a well known cause for lower performance of several machine learning algorithms. Data distributions on real world  are often skewed and rarely contains enough information to learn all the necessary features of the data domain. The natural phenomenon intensified by imbalanced learning is known as domain shift \cite{Quionero}. This happens when distributions on training and test stages are different and thus the model performs badly on unseen data.

This thesis presents an experimental study aimed to investigate the performance of class-imbalanced CNNs to white-box and black-box adversarial attacks. Through the use of the fast gradient sign method we test how imbalanced learning problem could affect the network robustness to such attacks. Understanding such threat vectors is of the utmost importance as several commercial systems rely heavily on models like convolutional neural networks and thus could be negatively impacted by such malicious attacks.


\section{Motivation}

Currently, there is no empirical evidence on the effectiveness of adversarial inputs on class-imbalanced CNNs. We designed a set of experiments to investigate the effects of both training sets with skewed distributions and the model's internal gradient information on the robustness of these networks to such attacks. The main contributions of this work are as follows: 
\begin{enumerate}
	\item To shed new light on how CNNs trained on imbalanced datasets are affected by adversarial attacks
	\item Evaluate the impact of transfer learning on imbalanced CNNs and how classes with similar set of features react to the perturbation caused by the gradient sign method
\end{enumerate}

The motivation for adversarial robustness comes largely from being able to shield image recognition systems from behaving unexpectedly. Experimental demonstrations of the effectiveness of adversarial attacks were carried out mainly by \cite{billovits,goodfellow2014,papernot2016} and have heightened the need for improvement on the current state of CNNs techniques. Developing robustness to such attacks has become of the utmost importance as many commercial applications are based on the same small group of models.

\section{Thesis Structure}

This thesis starts with a brief overview of the recent history of Neural Networks and the main related techniques and methods for using such networks on computer vision related tasks. Next chapter begins by introducing all the taxonomy around adversarial attacks followed by a theoretical discussion of domain shift in machine learning. Chapter 3 presents two variations of the gradient sign method and its different perturbation types. The fourth chapter presents an empirical study of machine learning transferability and provides a practical overview of different attacks to real world systems. The remaining chapters are concerned with explaining the experiment setup and presenting the achieved results, conclusions and related future work.
