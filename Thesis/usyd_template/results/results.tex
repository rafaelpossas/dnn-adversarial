\chapter{Results} \label{chap:results}
In this chapter we present the results from our experiments on different types of imbalanced datasets. We first start by showing the results on the balanced model so as to set the baseline of comparisons. A discussion then follows on the under-sampled and over-sampled cases and transfer learning results for perturbations with different models. The chapter is then concluded with the results for the cases where classes have similar features, and, therefore, share overlapping distributions.

\section{Baseline model}
Canonical models assume that every object in the dataset are sampled from similar distributions. However, in real-life situations, even though the number of samples is the same for each label, they could still be poorly represented by the lack of a clear structure. This often leads to differences in the output for each specific class \cite{krawczyk2016learning}. On this way, a superficially balanced dataset does not guarantee that the model will equally generalise across all classes.We use the results of the balanced network on adversarial attacks as the baseline
to evaluate whether imbalanced CNNs are more or less vulnerable to these malicious methods.

Table ~\ref{tbl:results} shows that the accuracy for all classes is drastically reduced when the balanced model is presented with adversarial inputs. Even though there is equally distributed number of samples for each class, the adversarial attack forces the domain shift of each individual sample towards different regions in space, causing a misclassification of the current label. Cat and dog classes are conatural adversaries according to \cite{papernot2016} and they presented the lowest accuracy of all classes with 11\% and 15\% respectively. As both labels have distributions that overlap, the sign method becomes more effective since the distance between them is smaller. 
\begin{table}
	\centering
	
	\begin{tabular}{lccccc}
		\toprule
		&\multicolumn{2}{c}{Different Model}
		&\multicolumn{3}{c}{Same Model}
		\\\cmidrule(r){2-3}\cmidrule(l){4-6}
		Class Label &Undersample &Oversample &Balanced &Undersample &Oversample \\
		\midrule
		0 - Airplane &60\%& 87\% &36\%& 19\%    & 61\% \\
		1 - Automobile &64\%& 91\% &23\%& 16\%    & 63\% \\
		2 - Bird &38\%& 73\% &20\%& 9.4\%    & 27\% \\
		3 - Cat &21\%& 72\% &11\%& 0.5\%    & 19\% \\
		4 - Deer &58\%& 80\% &20\%& 9.8\%    & 20\% \\
		5 - Dog &47\%& 76\% &15\%& 9\%    & 38\% \\
		6 - Frog &76\%& 88\% &27\%& 20\%    & 49\% \\
		7 - Horse &59\%& 88\% &20\%& 18\%    & 52\% \\
		8 - Ship &69\%& 89\% &37\%& 19\%    & 59\% \\
		9 - Truck &46\%& 87\% &49\%& 21\%    & 54\% \\
		\bottomrule
	\end{tabular}
	\caption{Results for the two different sources of perturbations along with the two different imbalanced datasets}
	\label{tbl:results}
\end{table}
\section{Under-sampling and over-sampling models}

The results on both under-sampled and over-sampled models has shown some interesting properties of adversarial attacks. 
Table ~\ref{tbl:results} illustrates that models with under-sampled datasets were more vulnerable than balanced models. Figure ~\ref{fig:relative_difference} shows the relative difference for all the three different networks (balanced, under-sampled and over-sampled). Values were calculated by finding the difference between the perturbed accuracy and the non-perturbed accuracy of each class model. They represent the percentage on which the initial accuracy was reduced. The under-sampled model had the higher relative difference on average, which shows that the imbalanced nature of the dataset ended-up increasing the vulnerability to adversarial attacks.

Even though the vast majority of results has shown higher vulnerability of the under-sampled models, some results are worth to discuss. For instance, the deer class has shown the same relative difference on both over-sampling and balanced model cases, which indicates that more samples are not necessarily adding more information to the class distribution. Further causality relationship could be established by looking at individual samples of the class to investigate if they are similar to one another so as to justify this result. However, if thats not the case, another hypothesis is that individual samples provide high quality information about the class and thus makes the model learning process on that class more effective. The dog and frog cases presented similar patterns between all three models while the cat class was the highest relative difference of all classes. 

Perturbation on the over-sampling case, has shown less effective on the majority of classes. The small push caused by our $\epsilon$ was not enough to move points to outside of their distributions. Objects of the over-sampled classes would need bigger steps in order to successfully create an adversary that leads to a wrong classification label. Accuracy for
most of the over-sampling cases was around 45\% and the relative difference was the lowest of all three models, which shows robustness of the target over-sampled class.For instance, airplane and automobile perturbations have reduced the initial accuracy by only 31.46\% and 32.26\$ respectively, which shows that over-sampling on these classes has helped to model to establish better decision boundaries during optimisation.

In order to better understand these results, we need to study how decision boundaries of models are established. Class imbalanced models are naturally affected by the false positive and false negative trade off shown on figure \ref{fig:class_dist}. The decision boundaries on such models favour the class with more samples and, hence, increases the accuracy for one class while decreasing for the other class. The area under the curve for misclassified examples on the under-sampled distribution is bigger, and it is caused by the suboptimal exploration of feature space of that class. This effect is exploited by adversaries as there is an increase on the misclassification rate of distributions with lower amplitude. An under-sample of a specific label causes its distribution to be squished into space and, hence, have less impact on the definition of decision boundaries.


The increased number of samples of the over-sampled label, causes the network to perform the aforementioned trade-off when optimizing its loss function. For instance, the decision boundary is chosen in order to minimize the total error of the network. The cost function is lower when the decision boundary minimizes the misclassification of the majority class, as there is a higher number of samples. This phenomenon is well explained by figure ~\ref{fig:class_dist}, and it could be one of the factors explaining the higher resilience of over-sampled networks.




\section{Transfer learning}
\begin{figure}
	\centering
	\includegraphics[scale=0.3]{rel_diff_graph.png}
	\caption{Relative difference for each model. Higher numbers means more vulnerability}
	\label{fig:relative_difference}
\end{figure}
Chapter 4 discussed different ways in which a new model could learn from existing ones. Black-box attack is when the attacker has no knowledge of the underlying model that he/she wants to attack and the best way to learn the gradient information is by querying the target and training a new model with its outputs. White-box attack is when the same model is used to extract the gradient for the gradient sign method. Our Black-box experiments has a slightly different configuration, as we use a modified version of the dataset to train a different model.


The use of a different model (black-box) for creating adversaries has shown less effective when compared to the same model (white-box) attack. As the overall gradient have not only different direction but also magnitudes, the attacked system has proven to be more robust. The experiment reveals that although gradient sign method is quite effective for fooling networks it does require a good amount of knowledge from the underlying training parameters so as to unleash its full potential.
\begin{figure}
	\centering
	\includegraphics[scale=0.32]{class_dist.png}
	\caption{Dataset imbalance causes models to perform adjustments of decision boundaries leading to an increase on accuracy of the majority class and decrease on the minority class.}
	\label{fig:class_dist}
\end{figure}
Attacking an under-sampled target with the gradient of a balanced model did not show to be as effective as using the same model's gradient. The average accuracy of the under-sampled attack with adversaries generated from a different network was 53.8\% while the same metric was 25.8\% for the white-box attack. Even that our training samples are within the same data domain, there are still huge differences on the gradients learned from each model. 

Due to the high dimensionality of CNN models, every training process learns different weights and biases that could equally generalise for all classes. As these are not guaranteed to converge to global optima, several different local optima could be found by the same optimisation procedure. Therefore, black-box adversary generation is not using the true target gradient directions, as they were submitted, in our case, to different optimisation stage and different dataset configuration.

Our experiments shows that gradient information is extremely important to gradient sign methods. Both the over-sampling of specific classes on the datasets and the model internal knowledge increases robustness to gradient sign methods. This shows that high quality datasets with equal number of samples per class are one way of avoiding the degradation of real-life systems performance when attacked by adversarial methods.

\section{Overlapping distributions}
The results for the balanced network on figure~\ref{fig:conf_matrix_full} shows that for the pairs cat/dog and automobile/truck there is already a natural misclassification between one another. For instance 13\% of dog samples were misclassified as cat in the original balanced model. Our experiment demonstrates that the adversarial attacks intensify this phenomena in only one of the classes of the pair. While for both under-sampled cat and truck the number of samples misclassified with the similar class has increased, the same did not happen with dog and automobile. Figure~\ref{fig:overlap} shows that cats are increasingly misclassified as dogs when under-sampling on the cat class is used. While on the cat under-sampling case the percentage of samples misclassified as dogs increased from 31\% to 39\%, the same number decreased from 38\% to 32\% on the dog under-sampling test.

\begin{figure}
	\centering
	\includegraphics[height=5.5cm]{overlapping_all.png}
	\caption{Under-sample on cat and truck increases misclassification to similar classes, while dog and automobile does not.}
	\label{fig:overlap}
\end{figure}

The most important features of the affected pair seems to overlap so as to this phenomena to happen. It is hard to reason with distributions on high-dimensional spaces, however, the results shows that linear behavior is achieved even on non-linear environments. This confirms previous works discoveries that although CNNs are made of non-linearities, they still perform linearly in most cases. Classes with overlapping distributions shows that decision boundaries of high-dimensional models are as fragile as the ones learned on linear models. One way to explain this is by looking at the activation functions of current CNNs architectures. They are usually made of RELU or TANH, which is a step function with somewhat linear behavior. The stacking of such functions create a high-dimensional linear space, where methods like the gradient sign are able to perform well by intentionally causing domain shift on data points that are close together in space.
