\chapter{Conclusion and future work} \label{chap:conclusion}
In this work we have presented how adversarial attacks using the gradient sign method with ascent perturbations perform on CNNs trained on imbalanced datasets using the same and different model gradients. Neural networks and convolutional neural networks were explained in details in chapter 1 followed on the next chapter by the adversarial attacks taxonomy and imbalanced learning problem implications to machine learning methods. Types of adversarial attacks were then presented, showing that black-box attacks are successful on real-world systems and the implications of model internal knowledge to the gradient sign method. Chapters 5 an d 6 were dedicated to showing the details of our experiment design and to present and discuss the results on all of our models.

Adversarial methods were proven to increase the domain shift effect on test datasets as it is explained by \cite{papernot2016transf}. We have shown that adversarial attacks are even more severe on datasets with under-sampled class labels and that the decision boundary trade-off on the over-sampled classes increases their robustness to adversarial examples. Labels with similar features have only shown higher vulnerability to the fast gradient sign methods in one of the classes of the pair. This specific result shows that similar classes might have degrees of similarities on which could be more or less exploited by the gradient method.  

Convolutional neural networks have been adopted more widely as the current techniques improve. However, understanding of such methods is hard as the number of degrees of freedom in the model hinders thorough analysis. Until now, most people  are used to see these models as black-boxes, since they can not reason about the causality relationship between the model performance and its internal knowledge. The highly non-linear structure and state of the art performance of CNNs do not shield them from one of the most common effects in machine learning models - the domain shift caused by unseen data points.

As several commercial applications rely on almost the same group of models, understanding of such properties is of extreme importance. Future work in this field could look further in datasets with a higher number of classes and more complex relationships between labels so as to not only confirm our insights but also discover new interesting properties of class imbalanced CNNs and adversarial attacks. Current applications looking to increase their robustness to adversarial attacks could use over-sampling techniques on critical labels so as to shield that label from malicious attacks.

This work sheds an important light on machine learning methods. Several real-life models are deeply concerned with possible vulnerabilities of their system, and studies on this field were being done for the past years. Still, the imbalance learning problem remains one of the big questions in machine learning. Current applications looking to increase their robustness to adversarial attacks could use over-sampling techniques on critical labels so as to shield that label from malicious attacks

